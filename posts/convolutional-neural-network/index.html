<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Convolutional Neural Network - Labyrinth</title><meta name=Description content="Sphinx of black quartz, judge my vow."><meta property="og:title" content="Convolutional Neural Network"><meta property="og:description" content="This article provides basic fundamentals of CNN."><meta property="og:type" content="article"><meta property="og:url" content="http://ykaros.github.io/posts/convolutional-neural-network/"><meta property="og:image" content="http://ykaros.github.io/images/logo/bauhaus.png"><meta property="article:published_time" content="2020-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-01T00:00:00+00:00"><meta property="og:site_name" content="Labyrinth"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://ykaros.github.io/images/logo/bauhaus.png"><meta name=twitter:title content="Convolutional Neural Network"><meta name=twitter:description content="This article provides basic fundamentals of CNN."><meta name=application-name content="Labyrinth"><meta name=apple-mobile-web-app-title content="Labyrinth"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://ykaros.github.io/posts/convolutional-neural-network/><link rel=next href=http://ykaros.github.io/posts/best-sci-fi/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Convolutional Neural Network","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/ykaros.github.io\/posts\/convolutional-neural-network\/"},"genre":"posts","wordcount":2073,"url":"http:\/\/ykaros.github.io\/posts\/convolutional-neural-network\/","datePublished":"2020-10-01T00:00:00+00:00","dateModified":"2020-10-01T00:00:00+00:00","publisher":{"@type":"Organization","name":"Ykaros"},"author":{"@type":"Person","name":"Ykaros"},"description":""}</script></head><body data-header-desktop=auto data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Labyrinth>Ykaros</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/photography/>Photography </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/Ykaros title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw" aria-hidden=true></i></a><a class=menu-item href=https://www.linkedin.com/in/jiazong/ title=LinkedIn rel="noopener noreffer" target=_blank><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Labyrinth>Ykaros</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/photography/>Photography</a><a class=menu-item href=/categories/>Categories</a><a class=menu-item href=/about/>About</a><a class=menu-item href=https://github.com/Ykaros title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw" aria-hidden=true></i></a><a class=menu-item href=https://www.linkedin.com/in/jiazong/ title=LinkedIn rel="noopener noreffer" target=_blank><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Convolutional Neural Network</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=http://www.ykaros.site/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Ykaros</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Machine Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2020-10-01>2020-10-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2073 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;10 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-convolutional-layer>1 Convolutional Layer</a></li><li><a href=#2-pooling-layer>2 Pooling Layer</a></li><li><a href=#3-fully-connected-layer>3 Fully Connected Layer</a></li><li><a href=#4-whats-more>4 What&rsquo;s more</a></li></ul></nav></div></div><div class=content id=content><p>Convolutional Neural Network (CNN, or ConvNet) is one of the most well-known deep learning algorithms which has been commonly implemented in a variety of machine learning tasks. The first presence of modern CNN can date back to late 1990s when LeCun et al. proposed a CNN architecture called LeNet based on MNIST database of handwritten digits, which could handle simple task of digit recognition. Despite its success on digit recognition, LeNet suffers from scalability problem due to limited computation resources, especially Graphics Processing Unit (GPU). In 2012, AlexNet achieved exceptional performance in the ImageNet Large Scale Visual Recognition Challenge, a project evaluating algorithms for object detection and classification based on ImageNet. Ever since the great success of AlexNet, CNN has been showing its unparalleled effectiveness in producing the state-of-the-art results on a wide range of problems in the area of machine learning, including image recognition, semantic segmentation, object detectionand video analysis.</p><p><figure><a class=lightgallery href=/posts/convolutional-neural-network/CNN.png title=CNN data-thumbnail=/posts/convolutional-neural-network/CNN.png data-sub-html="<h2>Typical CNN Architecture</h2><p>CNN</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/convolutional-neural-network/CNN.png data-srcset="/posts/convolutional-neural-network/CNN.png, /posts/convolutional-neural-network/CNN.png 1.5x, /posts/convolutional-neural-network/CNN.png 2x" data-sizes=auto alt=/posts/convolutional-neural-network/CNN.png width=3684 height=1356></a><figcaption class=image-caption>Typical CNN Architecture</figcaption></figure></p><p>The figure above indicates an end-to-end CNN architecture as an example. Accordingly, the input and output in CNN are a set of tensors, containing the information from the datasets. For colored image-based datasets, the input is a 2D array containing RGB channels of the image, a 3D array for video-based datasets and a 1D array for an audio input. The output is specifically defined by the task purpose, e.g., action categories represented by class numbers for action recognition.</p><p>CNN is composed of multiple layers of mathematical functionalities such as convolution and pooling. Convolutional layer consist of a set of learnable parameters and is introduced to extract features from raw images by downsampling the input images and detecting high-level features. Pooling is added to downsample the generated feature map further and denoise the feature map by removing anomaly values, e.g., by max pooling.</p><p>Every time we feed input (vectors) to the network, it will be transformed through a series of hidden layers such as convolutional, ReLU and pooling layers and the output will finally be generated from the last fully connected layer. The hidden layers will perform feature extraction while the fully connected layer will produce the desired results such as classified categories from the extracted features in classification tasks.</p><p>Nowadays, CNN has achieved abundant impressive results in a variety of machine learning tasks, particularly in computer vision area. CNN is able to learn representations from the tensor data such as images, and recently it has shown its compelling performance improvement compared with other approaches. Since CNN has been proved a feasible and efficient method to generate and differentiate between features, it is often implemented for feature generation and classification.</p><h2 id=1-convolutional-layer>1 Convolutional Layer</h2><p>The most typical and fundamental component in CNN is convolutional layer. Each convolutional layer is a unit whose parameters consist of a set of learnable kernels and ﬁlters where we can assign learnable parameters (weights and biases) to differentiate one object from another given the input and the parameters can be trained with gradient descent and updated periodically. It consists of linear and non-linear operations, i.e., convolution operation and activation function. Parameter sharing is another critical concept in convolution process, the idea behind which is to accelerate the training process by with less learnable parameters.</p><p>Convolution is one type of linear operation used to extract feature from the input, where the unit takes in input and calculates convolved results by filtering the vector with kernel on receptive fields. As has been described, this convolution will reduce the in-plane dimension compared with the input tensor. Thus, padding such as zero padding is applied to address the issue so that the outermost element of the input tensor can be convolved and the same dimension can be kept after convolution. Moreover, stride, another down-sampling technique to reduce the in-plane dimension by distance between two successive kernel positions, is often involved in the convolution process. Take an RGB image ($\operatorname{dim}($I$)=\left(n_{H}, n_{W}, n_{C}\right)$) as an example, the convolution By kernel ($ \operatorname{dim}($K$)=\left(f, f, n_{C}\right)$) is deﬁned as:
\begin{equation}
\operatorname{conv}\left(I, K\right)_{x, y} = \sum_{i=1}^{n_{H}} \sum_{j=1}^{n_{W}} \sum_{k=1}^{n_{C}} K_{i, j, k} I_{x+i-1, y+j-1, k}
\end{equation}</p><p>After convolution, the dimension of the feature map with stride $s$ and padding $p$ is:
<data id=id-1 data-raw></data>where $|x|$ is the floor function of $x$.</p><p>After the convolution, feature maps representing different characteristics of the input tensors will be generated and passed onto next layer, which simulates the response of neuron to a specific stimulus. In the process of convolution, multiple kernels will be applied on the input so that feature maps containing diverse characteristics can be extracted from the input.</p><p><figure><a class=lightgallery href=/posts/convolutional-neural-network/Convolution.png title="Convolutional Layer" data-thumbnail=/posts/convolutional-neural-network/Convolution.png data-sub-html="<h2>Illustration of a Convolutional Layer. Multiple kernels (filters) are applied on the input image or input feature map to generate the output features maps by convolution. </h2><p>Convolutional Layer</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/convolutional-neural-network/Convolution.png data-srcset="/posts/convolutional-neural-network/Convolution.png, /posts/convolutional-neural-network/Convolution.png 1.5x, /posts/convolutional-neural-network/Convolution.png 2x" data-sizes=auto alt=/posts/convolutional-neural-network/Convolution.png width=734 height=441></a><figcaption class=image-caption>Illustration of a Convolutional Layer. Multiple kernels (filters) are applied on the input image or input feature map to generate the output features maps by convolution.</figcaption></figure></p><p>Activation function is the non-linear transformation in the network to enable the learning of complex patterns by determining the firing of neurons and their connections to subsequent neurons. It is desirable to include such function in multi-layer networks as they introduce non-linearities which will produce a nonlinear decision boundary to distinguish between inputs. Activation function is able to differentiate between in-plane features and lead to better patterns representation from the input data. The
appropriate activation function can improve the learning process by reducing the training time. Among the most common activation functions, Rectiﬁed Linear Unit (ReLU) has been shown to be preferable since it leads to faster training and better performance than others such as sigmoid and hyperbolic tangent (tanh) function.
\begin{equation}
f(x)=\max (0, x)
\end{equation}
To overcome the potential disadvantage of non-differentiable problem of ReLU at zero, Leaky ReLU is proposed so that the weights can be updated in negative region.</p><p>The objective of convolution layer is to extract high-level features such as textures from the input image, which often involves multiple convolutional layers where the first several layers are supposed to capture some low-level features like edges and the last several ones will capture high-level features like textures.</p><h2 id=2-pooling-layer>2 Pooling Layer</h2><p>Similar to convolutional layer, pooling layer is responsible for down-sampling the representation size spatially, which will
build connection between adjacent local receptive field, and reduce the number of in-plane dimension further. Except from dimensionality reduction, it will also help extract rotational and positional invariant features from the input. There are two common pooling operations, including a linear operation average pooling and non-linear operation max pooling. Regularly, max pooling can outperform average pooling since it can denoise the representation by ignoring the anomaly values.</p><p><figure><a class=lightgallery href=/posts/convolutional-neural-network/Pooling.png title="Pooling Layer" data-thumbnail=/posts/convolutional-neural-network/Pooling.png data-sub-html="<h2>Pooling Layer. If layer $n$ is a pooling layer and given $4$ feature maps from  the previous layer, every feature map is pooled and downsampled separately. Each unit in one of the $4$ output feature maps represents the average or the maximum value within a receptive field of the corresponding previous feature map.</h2><p>Pooling Layer</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/convolutional-neural-network/Pooling.png data-srcset="/posts/convolutional-neural-network/Pooling.png, /posts/convolutional-neural-network/Pooling.png 1.5x, /posts/convolutional-neural-network/Pooling.png 2x" data-sizes=auto alt=/posts/convolutional-neural-network/Pooling.png width=715 height=481></a><figcaption class=image-caption>Pooling Layer. If layer $n$ is a pooling layer and given $4$ feature maps from the previous layer, every feature map is pooled and downsampled separately. Each unit in one of the $4$ output feature maps represents the average or the maximum value within a receptive field of the corresponding previous feature map.</figcaption></figure></p><h2 id=3-fully-connected-layer>3 Fully Connected Layer</h2><p>After the feature extraction process by the previous layers, there will be one or more fully connected layers, also known as dense layers, which typically have the same number of output nodes as the number of classes and will perform high-level reasoning by building full connections to all activations from previous layers. The softmax function is often attached into the fully connected layer, which is a function that transforms a vector of $K$ real values into a vector of $K$ real values that sum to $1$. By adding fully connected layers, the network is able to learn a possibly non-linear function in the space of features generated by previous operations.</p><h2 id=4-whats-more>4 What&rsquo;s more</h2><p>The motivation for deep CNN architectures is based on the assumption that the deeper network is able to approximate the target function with more nonlinear mappings and diverse feature hierarchies and deeper networks can represent certain classes of function more efficiently than shallower ones. However, training a deeper networks is always a challenging task, and there have been a number of attempts trying to solve issues brought by deeper network such as gradient diminishing. Deeper CNN can theoretically perform well on complicated tasks. However, they may suffer issues brought by the increase of depth, including performance degradation, gradient vanishing, or explosion problems.</p><p>ResNet, short for Residual Network, is one type of CNN that was firstly introduced in 2015, which revolutionised CNN by introducing the concept of residual learning in CNN training process to address the problems brought by deeper network. The most significant intuition behind ResNet is by introducing residual block which creates shortcut that can skip one or more layers. The reason why skip connection is introduced is that we want to overcome gradients vanishing by reusing activations from a previous layers so that the adjacent layers can learn weights. Apart from avoiding gradient vanishing, residual block will also speed the training process since skipping can reduce the network and lead to less parameter that require back propagation. Consequently, training much deeper networks becomes possible and the performance of deeper network is supposed to be at least as good as a shallower one.</p><p><figure><a class=lightgallery href=/posts/convolutional-neural-network/Residual.png title="Residual Block" data-thumbnail=/posts/convolutional-neural-network/Residual.png data-sub-html="<h2>A Normal Block (left) and a Residual Block (right)</h2><p>Residual Block</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/convolutional-neural-network/Residual.png data-srcset="/posts/convolutional-neural-network/Residual.png, /posts/convolutional-neural-network/Residual.png 1.5x, /posts/convolutional-neural-network/Residual.png 2x" data-sizes=auto alt=/posts/convolutional-neural-network/Residual.png width=629 height=468></a><figcaption class=image-caption>A Normal Block (left) and a Residual Block (right)</figcaption></figure></p><p>As the figure indicates, there is an extra identity mapping from the input to the output so that the output $y_l$ becomes $f(x_l, w_l) + x_{l}$ instead of $f(x_l, w_l)$ by residual block, where $x_{l} = ReLU(y_{l-1})$.
The intuition behind the residual block is that sometimes identity mapping is optimal and it would be difficult for a set of non-linear layers to fit an identity mapping so that the residual blocks make it easier to optimize the residual function $f(x_l, w_l) + x_{l}$ than $f(x_l, w_l)$. Moreover, the involvement of residual block also reduces the error rate brought by much deeper network structure.</p><p>The reason for choosing ResNet over other popular CNN architectures is that ResNet can be more efficient as it requires less time in training with less parameters to learn and there are a variety of ResNet which is flexible for us to conduct further research. Since there is action recognition task on per-frame data, ResNet can be an effective approach.</p><p>CNN has been developing in computer vision area in decades. One of the most popular approaches of CNN-based action recognition is two-stream CNN with 2D convolutional kernels, which combines RGB and incorporated optical flow frames as appearance and motion information. The efficiency of two-stream CNN is proved by increase of action recognition accuracy. Thereafter, a number of methods based on the two-stream CNN are proposed to
improve action recognition performance.</p><p>CNN with spatial-temporal 3D convolutional kernels, which can learn spatial-temporal feature representation from raw videos, was actually proposed many years ago but it cannot achieve a compelling results as two-stream CNN does as a result of relatively small-scale of existing video datasets for optimizing the huge number of parameters in 3D CNN, which are much more than those in a 2D CNN. With more large-scale video datasets released, 3D CNN has recently achieved impressive performance on a variety of tasks based on large-scale datasets. Despite training a 3D CNN requires more computation resources and can be difficult to optimize, the use of recent large-scale video datasets enables the training and significantly improves their performance. 3D ResNet is proposed based on ResNet. The main difference is that 3D ResNet has different number of dimensions of convolutional kernels and pooling, which is able to take a sequence of images as input.</p><p><figure><a class=lightgallery href=/posts/convolutional-neural-network/3DCNN.png title="3D CNN" data-thumbnail=/posts/convolutional-neural-network/3DCNN.png data-sub-html="<h2>Typical 3D CNN Architecture</h2><p>3D CNN</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/convolutional-neural-network/3DCNN.png data-srcset="/posts/convolutional-neural-network/3DCNN.png, /posts/convolutional-neural-network/3DCNN.png 1.5x, /posts/convolutional-neural-network/3DCNN.png 2x" data-sizes=auto alt=/posts/convolutional-neural-network/3DCNN.png width=1228 height=284></a><figcaption class=image-caption>Typical 3D CNN Architecture</figcaption></figure></p><p>As has been shown, the input is a sequence of frames by time order and the output is the most possible categories of the frame sequence. By adding extra dimension of kernels and pooling, the network is able to learn not only spatial features but also temporal features from the input sequence so that the relationship within the sequence can be detected and utilized. These 3D CNN are intuitively effective
because such 3D convolution can be implemented to directly extract spatial-temporal features from raw videos rather than generating and combining optical flow in two-stream CNN.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2020-10-01</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/best-sci-fi/ class=next rel=next title="Best Sci-fi Films of All Time">Best Sci-fi Films of All Time<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://www.ykaros.site/ target=_blank>Ykaros</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"data":{"id-1":"\n\\begin{equation}\n    \\operatorname{dim}(\\operatorname{con} v(I, K))=\\left\\{\\begin{array}{ll}\n\\left(\\left\\lfloor\\frac{n_{H}+2 p-f}{s}+1\\right\\rfloor,\\left\\lfloor\\frac{n_{\\mathrm{W}}+2 p-f}{s}+1\\right\\rfloor\\right) \u0026 s\u003e0 \\\\\n\\left(n_{H}+2 p-f, n_{W}+2 p-f\\right) \u0026 s=0\n\\end{array}\\right.\n\\end{equation}\n"},"lightgallery":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type=text/javascript src=/js/theme.min.js></script></body></html>