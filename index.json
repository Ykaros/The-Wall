[{"categories":["misc"],"content":"Recently I made a sterling silver pendant (a replica based on classic Rick Owens pentagram) and I noticed the appearance of iridescent colors when oxidizing it, from light yellow to golden to a mixture tones of red violet and teal to black eventually. Oxidation of Silver It turns out to be the result of Thin Film Interference, which also accounts for the phenomenon of iridescent soap bubbles. Wikipedia has explained this pretty well. To keep it short here, a thin oxide layer is gradually thickening when the silver is heated, forming two reflection surfaces (air-oxide interface and oxide-silver interface). The thickness of the oxide layer further determines optical path difference (as straightforwardly calculated on Wikipedia) for either constructive interference (enhance a certain color) or destructive interference (diminish a certain color). Therefore, a changing of colors is observed when oxidizing the silver. Intuitively, shorter wavelengths should be observed before longer ones since they require smaller OPD for constructive interference. But wait! Shouldn’t that mean violet and blue tones are supposed to appear earlier than other colors? I’m pretty sure I saw the light yellow color first. Unfortunately, I cannot find a definitive explanation to it but I have some theories. According to the condition of constructive interference, OPD needs to be equal or very close to the multiple of the wavelength. So it might be violet and blue cover less wavelength range so there might be cooler hues but they diminish fast. However, this is apparently wrong because the wavelength of violet and blue ranges roughly from 400-500 nm while yellow only spans 560-600 nm. Human eyes are much more sensitive to yellow-green or similar hues so that the violet and blue light actually appears but they are less noticeable. Sensitivity of Human Eye to Visible Light Sterling silver contains 7.5% copper which is positioned before silver in view of metal reactivity series and might account for the light yellow color at first. I personally would go with this hypothesis at the moment. Hopefully I’ll find a concrete explanation to complete this section in future. Anyway, I had fun cutting fingers :( silver when making this pentagram and will make more accessories when I have time. ","date":"2024-11-16","objectID":"/posts/oxidation-of-silver/:0:0","tags":null,"title":"Oxidation of Silver","uri":"/posts/oxidation-of-silver/"},{"categories":null,"content":"Qualifications Sponsorship: Currently on OPT-F1, will apply STEM-OPT soon. Will need sponsorship for H-1B. Willing to relocate and work onsite daily ","date":"2024-01-20","objectID":"/resume/:1:0","tags":null,"title":"Self-Introduction","uri":"/resume/"},{"categories":null,"content":"Resume Here is my resume: Click to download ","date":"2024-01-20","objectID":"/resume/:2:0","tags":null,"title":"Self-Introduction","uri":"/resume/"},{"categories":null,"content":"Cover Letter Being a recent grad and active job seeker, I’ve acquired knowledge in Cryptography, Machine Learning, Security, Web Development, Web Crawling and System Design from working on various independent and group projects. I can still recall the excitement of developing a fully operational prototype management tool implemented by Python and SQL during my sophomore year. Faced with challenges demanding repetitive work, I always came up with a strategy to utilize scripts to fulfill the task, which leads me to mastering web crawling and script languages like Python and Go. Based on my abundant experiences in machine learning and data analysis, I can claim with confidence that I am capable of processing any datasets and gaining valuable insights. Learning cryptography now becomes a current interest for me, starting from a brain teaser to exercise my intellect to enhancing my knowledge about how this essential cornerstone shapes the modern technological landscape. The diversity of my background and interests help me to analyze tasks from a more holistic perspective by connecting all the dots. Furthermore, I enjoy devising logical and analytical approach towards solving problems from scratch to optimal solution. After graduation, I’ve been working on several volunteer projects and personal projects to learn and practice my understanding towards system design. With a deep-rooted curiosity about the universe and profound passion for technology, I thrive on exploring cutting-edge technological breakthroughs. I’m strongly eager to apply academic knowledge and hands-on experience to contribute to a dynamic, innovative and diverse team. I also have abundant experiences in teaching/mentoring as furnished below: Teaching Assistant during my Bachelor’s degree in Australian National University for course Advanced Computing R\u0026D Methods, where I acted as a supervisor for a group of students by organizing weekly seminars, devising assignments, assessing their progress and providing feedback. Teaching Assistant during my Master’s degree in Johns Hopkins University where I was tutoring for several courses, during which I helped organize course materials, grade assignments and review semester-long group projects. ","date":"2024-01-20","objectID":"/resume/:3:0","tags":null,"title":"Self-Introduction","uri":"/resume/"},{"categories":["Tools"],"content":"1 Auto-join WiFi Firstly, modify the network interface file sudo vim /etc/network/interfaces by adding these lines: auto wlan0 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf iface default inet dhcp Then, configure the WiFi connection details by modifying sudo vim /etc/wpa_supplicant/wpa_supplicant.conf network={ ssid=\"NETWORK_NAME\" psk=\"NETWORK_PASSWORD\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN } Now the Raspi should automatically join the WiFi upon booting. ","date":"2023-11-20","objectID":"/posts/raspberry-pi-101/:1:0","tags":null,"title":"Raspberry Pi 101","uri":"/posts/raspberry-pi-101/"},{"categories":["Tools"],"content":"2 Find Raspi’s IP Address I do not have an extra monitor for the Pi and I do not feel like an extra cable messing up my desktop so I have to find the ip address through my PC by sudo nmap -sn 192.168.0.1/24 where 192.168.0.1 is the router’s ip and 0/24 ranges from 0 to 255. From the list of found IPs, we can determine the Raspi’s IP address ","date":"2023-11-20","objectID":"/posts/raspberry-pi-101/:2:0","tags":null,"title":"Raspberry Pi 101","uri":"/posts/raspberry-pi-101/"},{"categories":["Tools"],"content":"3 SSH without Password ssh-copy-id is a good tool to achieve login without password everytime. Simply brew install ssh-copy-id and then use it ssh-copy-id user@hostname. Now ssh to the Raspi should not require password ssh user@hostname. Finally, modify ~/.ssh/config by adding Host pi HostName [IP of Raspi] User [username] Port 22 ","date":"2023-11-20","objectID":"/posts/raspberry-pi-101/:3:0","tags":null,"title":"Raspberry Pi 101","uri":"/posts/raspberry-pi-101/"},{"categories":["Tools"],"content":"4 Tools Privoxy Pi-hole PiVPN ","date":"2023-11-20","objectID":"/posts/raspberry-pi-101/:4:0","tags":null,"title":"Raspberry Pi 101","uri":"/posts/raspberry-pi-101/"},{"categories":["Networks"],"content":"DNS (Domain Name System), is a hierarchical and distributed naming system used to convert human-readable domain names into machine-readable IP addresses. We can use dig www.ykaros.site +trace to show a detailed step-by-step view of how the DNS resolution process unfolds, starting from root DNS servers down to the authoritative DNS servers. ","date":"2023-10-01","objectID":"/posts/dns/:0:0","tags":null,"title":"Domain Name System","uri":"/posts/dns/"},{"categories":["Networks"],"content":"Root DNS Server The first part shows details for root DNS servers where they start with a dot (.) represent the root DNS servers and there are 13 of them, from a.root-servers.net. to m.root-servers.net. The last line is an RRSIG (Resource Record Signature) record, used for providing cryptographic signatures for the associated NS records. Let’s dig into it: 8: The algorithm used to sign (RSA/SHA-256) 0: Number of labels of the RRSET 518400: The TTL (Time to Live) of the RRSIG record (in seconds) 20240116170000: The expiration date 20240103160000: The inception date 30903: The key tag .: Signer’s name b1uPMq5/1: The Signature ; \u003c\u003c\u003e\u003e DiG 9.10.6 \u003c\u003c\u003e\u003e www.ykaros.site +trace ;; global options: +cmd . 342267 IN NS c.root-servers.net. . 342267 IN NS h.root-servers.net. . 342267 IN NS a.root-servers.net. . 342267 IN NS l.root-servers.net. . 342267 IN NS e.root-servers.net. . 342267 IN NS b.root-servers.net. . 342267 IN NS i.root-servers.net. . 342267 IN NS f.root-servers.net. . 342267 IN NS d.root-servers.net. . 342267 IN NS g.root-servers.net. . 342267 IN NS k.root-servers.net. . 342267 IN NS j.root-servers.net. . 342267 IN NS m.root-servers.net. . 428658 IN RRSIG NS 8 0 518400 20240116170000 20240103160000 30903 . b1uPMq5/173mbq1JX9U2W38gSTzWkpiDVsA3sMPBKZIXhr61h89W3c96 oyd+REdRsdVKV2X14bpXDhYCrteHHv8gmXRM+8GjKepPORvUzVo82jP4 xT5dQZTSFVbkFb7qH7gZLgDPiQxrwcBqySQlAc2xhcEOVEIXWf5Pek89 96palcC0d4Avx3wO/M883dxFKpW+Y5Z9aNoJDlFoS4FhNaQqHBuhRvxz c/TeyiL9O2oWDqCCe4qVKB/e2BQhi21DMgt5Cx6Q2CS/jIcr4NgOBk3c X8UJ6GfTAc8Zuw9tupQSMssx4ZiIEnqouyhy5RqCXNG7bpy/Shsm6tZ2 xBtHzA== ","date":"2023-10-01","objectID":"/posts/dns/:1:0","tags":null,"title":"Domain Name System","uri":"/posts/dns/"},{"categories":["Networks"],"content":"Top Level Domain (TLD) DNS Server Then the root servers send queries to TLD servers to query site.. ;; Received 1109 bytes from 10.173.91.33#53(10.173.91.33) in 55 ms site. 172800 IN NS a.nic.site. site. 172800 IN NS b.nic.site. site. 172800 IN NS e.nic.site. site. 172800 IN NS f.nic.site. site. 86400 IN DS 51676 8 1 90DDBEEEB973B0F8719ED763FB6EEDE97C73ABF5 site. 86400 IN DS 51676 8 2 883175F6F5C68EA81563B62D1B2B79B6A997D60DC6E20CC70AFD0CD6 B7E82F62 site. 86400 IN RRSIG DS 8 1 86400 20240117170000 20240104160000 30903 . mmwjDLH4smT/Luwcxp73hnrDYvKVvKBhTUq85PEpJvYLJjJ+8iROo/hR Und3191XZOvf2HMP3UHH5s+9nigGJ6WnYS1rXn2qFGJtiVTScO9+tVPf 8WxXVB8wpmniioDDdN6WwHaC+PrGbQd/RX+ZlSeIXI+rmPt1o67zhttY /J+toj6rZiCRZEPl0yoL0FeeVOS7s2KIojFL0se99gEks13D/urnSuFz mCMtVCrnpoGpfUV498EUqMCXQexCNOvnZdy+rE2r8CqmwsDYY+SSDnTl PJUXVIcLHQ+ZRmebY9H8tZVkXV0dwXOt7C8UcKRRZJu4jrBs0rSwNVjd 4M47pw== ","date":"2023-10-01","objectID":"/posts/dns/:2:0","tags":null,"title":"Domain Name System","uri":"/posts/dns/"},{"categories":["Networks"],"content":"Authoritative Name Server Once the resolver obtains the authoritative name servers for the top-level domain (TLD), it will continue the trace by querying the authoritative servers for each subsequent level of the domain until it reaches the authoritative name servers for the specified subdomain. Finally, it states that the domain “www.ykaros.site” is a Canonical Name (CNAME) record pointing to “ykaros.github.io.”. ;; Received 659 bytes from 198.97.190.53#53(h.root-servers.net) in 30 ms ykaros.site. 3600 IN NS dns1.registrar-servers.com. ykaros.site. 3600 IN NS dns2.registrar-servers.com. . . . ;; Received 590 bytes from 185.24.64.61#53(b.nic.site) in 17 ms www.ykaros.site. 1799 IN CNAME ykaros.github.io. ;; Received 74 bytes from 156.154.132.200#53(dns1.registrar-servers.com) in 13 ms ","date":"2023-10-01","objectID":"/posts/dns/:3:0","tags":null,"title":"Domain Name System","uri":"/posts/dns/"},{"categories":["Development"],"content":"This short article provides how to deploy a Hugo site where the github actions come in handy. (It is not a tutorial to build a site with Hugo) Before the technical part, I would like to demystify the motivation for building this site which, as depicted, should act like a chronicle of thoughts for reflecting, documenting and organizing sparks of inspiration over time. Additionally, it serves as a portfolio that exhibits a diverse range of work. Now it’s time for github actions. ","date":"2023-06-10","objectID":"/posts/github-actions/:0:0","tags":null,"title":"Github Actions for Hugo Page","uri":"/posts/github-actions/"},{"categories":["Development"],"content":"Theme The selected theme runs as a submodule in the source repo so that Hugo is able to build the site from the container by solely copying the markdown files. git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt To remove a previously added submodule, this post can help. ","date":"2023-06-10","objectID":"/posts/github-actions/:1:0","tags":null,"title":"Github Actions for Hugo Page","uri":"/posts/github-actions/"},{"categories":["Development"],"content":"Deployment Note There are two repos in my implementation: one is to store markdown and config files and the other is for publishing the static site. But you can definitely achieve this by one repo as described in Hugo Docs Thanks to actions-hugo, we have a simple solution for deploying a Hugo site with github actions. Generate deploy keys by ssh-keygen -t rsa -b 4096 -C “$(git config user.email)” -f deploy-key -N “\" With the key pair, we store private key and public key correspondingly in source repo (Actions secrets and variables) and page repo (Deploy keys). Put workflow file in .github/workflows/ in source repo. name: Remote Deployment on: push: branches: - master jobs: build-deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: Checkout submodules shell: bash run: | auth_header=\"$(git config --local --get http.https://github.com/.extraheader)\" git submodule sync --recursive git -c \"http.extraheader=$auth_header\" -c protocol.version=2 submodule update --init --force --recursive --depth=1 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 0.68.3 extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.DEPLOY_KEY }} external_repository: Ykaros/ykaros.github.io publish_branch: master publish_dir: ./public ","date":"2023-06-10","objectID":"/posts/github-actions/:2:0","tags":null,"title":"Github Actions for Hugo Page","uri":"/posts/github-actions/"},{"categories":["Development"],"content":"Submodule Update Recent Update I made some changes to the submodule recently but the they are not reflected when I push the changes to the submodule. To make sure the latest commits submodule are tracked by the parent repository, simply push the latest submodule by git add themes/LoveIt git commit -m \"submodule sync\" git push origin More information can be found at this post ","date":"2023-06-10","objectID":"/posts/github-actions/:3:0","tags":null,"title":"Github Actions for Hugo Page","uri":"/posts/github-actions/"},{"categories":["Cinematography"],"content":"I am an enthusiast of sci-fi films as they provide infinite possibilities for thought-provoking narratives that can intersect with different genres. Here is the personally curated list of greatest sci-fi films, in chronological order. I will add brief review to each of them some time. Disclaimer: Everyone knows Star Wars, Star Trek, and Doctor Who so they are not specifically mentioned. The list will definitely go longer, but not significantly. Le voyage dans la lune — 1902 Metropolis — 1927 La jetée — 1962 Alphaville — 1965 Seconds — 1966 2001: A Space Odyssey — 1968 Je t’aime, je t’aime — 1968 Planet of the apes — 1968 A Clockwork Orange — 1971 Solaris — 1972 La Planète sauvage — 1973 Stalker — 1979 Back to the Future — 1985 Brazil — 1985 Cuo wei — 1986 Short Circuit — 1986 On the Silver Globe — 1988 Terminator 1/2 — 1991 Until the End of the World — 1991 Contact — 1997 Face/Off — 1997 Gattaca — 1997 Lola Rennt — 1998 The Truman Show — 1998 Bicentennial Man — 1999 Artificial Intelligence: AI — 2001 K-PAX — 2001 The Matrix 1/2/3 — 2003 The Butterfly Effect — 2004 I, Robot — 2004 V for Vendetta — 2005 Children of men — 2006 Paprika — 2006 Avatar — 2009 District 9 — 2009 Moon — 2009 Watchmen — 2009 Inception — 2010 Source Code — 2011 Coherence — 2013 Her — 2013 Snowpiercer — 2013 Interstellar — 2014 Mad Max: Fury Road — 2015 Arrival — 2016 Blade Runner — 1982/2017 Westworld Season 1/2 — 2018 Calls — 2021 Dune — 2021 Love, Death \u0026 Robots Season 1/3 — 2022 The Wandering Earth 1/2 — 2023 ","date":"2023-06-05","objectID":"/posts/best-sci-fi/:0:0","tags":null,"title":"Best Sci-fi Films of All Time","uri":"/posts/best-sci-fi/"},{"categories":["Machine Learning"],"content":"Convolutional Neural Network (CNN, or ConvNet) is one of the most well-known deep learning algorithms which has been commonly implemented in a variety of machine learning tasks. The first presence of modern CNN can date back to late 1990s when LeCun et al. proposed a CNN architecture called LeNet based on MNIST database of handwritten digits, which could handle simple task of digit recognition. Despite its success on digit recognition, LeNet suffers from scalability problem due to limited computation resources, especially Graphics Processing Unit (GPU). In 2012, AlexNet achieved exceptional performance in the ImageNet Large Scale Visual Recognition Challenge, a project evaluating algorithms for object detection and classification based on ImageNet. Ever since the great success of AlexNet, CNN has been showing its unparalleled effectiveness in producing the state-of-the-art results on a wide range of problems in the area of machine learning, including image recognition, semantic segmentation, object detectionand video analysis. Typical CNN Architecture The figure above indicates an end-to-end CNN architecture as an example. Accordingly, the input and output in CNN are a set of tensors, containing the information from the datasets. For colored image-based datasets, the input is a 2D array containing RGB channels of the image, a 3D array for video-based datasets and a 1D array for an audio input. The output is specifically defined by the task purpose, e.g., action categories represented by class numbers for action recognition. CNN is composed of multiple layers of mathematical functionalities such as convolution and pooling. Convolutional layer consist of a set of learnable parameters and is introduced to extract features from raw images by downsampling the input images and detecting high-level features. Pooling is added to downsample the generated feature map further and denoise the feature map by removing anomaly values, e.g., by max pooling. Every time we feed input (vectors) to the network, it will be transformed through a series of hidden layers such as convolutional, ReLU and pooling layers and the output will finally be generated from the last fully connected layer. The hidden layers will perform feature extraction while the fully connected layer will produce the desired results such as classified categories from the extracted features in classification tasks. Nowadays, CNN has achieved abundant impressive results in a variety of machine learning tasks, particularly in computer vision area. CNN is able to learn representations from the tensor data such as images, and recently it has shown its compelling performance improvement compared with other approaches. Since CNN has been proved a feasible and efficient method to generate and differentiate between features, it is often implemented for feature generation and classification. ","date":"2020-10-01","objectID":"/posts/convolutional-neural-network/:0:0","tags":null,"title":"Convolutional Neural Network","uri":"/posts/convolutional-neural-network/"},{"categories":["Machine Learning"],"content":"1 Convolutional Layer The most typical and fundamental component in CNN is convolutional layer. Each convolutional layer is a unit whose parameters consist of a set of learnable kernels and ﬁlters where we can assign learnable parameters (weights and biases) to differentiate one object from another given the input and the parameters can be trained with gradient descent and updated periodically. It consists of linear and non-linear operations, i.e., convolution operation and activation function. Parameter sharing is another critical concept in convolution process, the idea behind which is to accelerate the training process by with less learnable parameters. Convolution is one type of linear operation used to extract feature from the input, where the unit takes in input and calculates convolved results by filtering the vector with kernel on receptive fields. As has been described, this convolution will reduce the in-plane dimension compared with the input tensor. Thus, padding such as zero padding is applied to address the issue so that the outermost element of the input tensor can be convolved and the same dimension can be kept after convolution. Moreover, stride, another down-sampling technique to reduce the in-plane dimension by distance between two successive kernel positions, is often involved in the convolution process. Take an RGB image ($\\operatorname{dim}($I$)=\\left(n_{H}, n_{W}, n_{C}\\right)$) as an example, the convolution By kernel ($ \\operatorname{dim}($K$)=\\left(f, f, n_{C}\\right)$) is deﬁned as: \\begin{equation} \\operatorname{conv}\\left(I, K\\right)_{x, y} = \\sum_{i=1}^{n_{H}} \\sum_{j=1}^{n_{W}} \\sum_{k=1}^{n_{C}} K_{i, j, k} I_{x+i-1, y+j-1, k} \\end{equation} After convolution, the dimension of the feature map with stride $s$ and padding $p$ is: where $|x|$ is the floor function of $x$. After the convolution, feature maps representing different characteristics of the input tensors will be generated and passed onto next layer, which simulates the response of neuron to a specific stimulus. In the process of convolution, multiple kernels will be applied on the input so that feature maps containing diverse characteristics can be extracted from the input. Illustration of a Convolutional Layer. Multiple kernels (filters) are applied on the input image or input feature map to generate the output features maps by convolution. Activation function is the non-linear transformation in the network to enable the learning of complex patterns by determining the firing of neurons and their connections to subsequent neurons. It is desirable to include such function in multi-layer networks as they introduce non-linearities which will produce a nonlinear decision boundary to distinguish between inputs. Activation function is able to differentiate between in-plane features and lead to better patterns representation from the input data. The appropriate activation function can improve the learning process by reducing the training time. Among the most common activation functions, Rectiﬁed Linear Unit (ReLU) has been shown to be preferable since it leads to faster training and better performance than others such as sigmoid and hyperbolic tangent (tanh) function. \\begin{equation} f(x)=\\max (0, x) \\end{equation} To overcome the potential disadvantage of non-differentiable problem of ReLU at zero, Leaky ReLU is proposed so that the weights can be updated in negative region. The objective of convolution layer is to extract high-level features such as textures from the input image, which often involves multiple convolutional layers where the first several layers are supposed to capture some low-level features like edges and the last several ones will capture high-level features like textures. ","date":"2020-10-01","objectID":"/posts/convolutional-neural-network/:1:0","tags":null,"title":"Convolutional Neural Network","uri":"/posts/convolutional-neural-network/"},{"categories":["Machine Learning"],"content":"2 Pooling Layer Similar to convolutional layer, pooling layer is responsible for down-sampling the representation size spatially, which will build connection between adjacent local receptive field, and reduce the number of in-plane dimension further. Except from dimensionality reduction, it will also help extract rotational and positional invariant features from the input. There are two common pooling operations, including a linear operation average pooling and non-linear operation max pooling. Regularly, max pooling can outperform average pooling since it can denoise the representation by ignoring the anomaly values. Pooling Layer. If layer $n$ is a pooling layer and given $4$ feature maps from the previous layer, every feature map is pooled and downsampled separately. Each unit in one of the $4$ output feature maps represents the average or the maximum value within a receptive field of the corresponding previous feature map. ","date":"2020-10-01","objectID":"/posts/convolutional-neural-network/:2:0","tags":null,"title":"Convolutional Neural Network","uri":"/posts/convolutional-neural-network/"},{"categories":["Machine Learning"],"content":"3 Fully Connected Layer After the feature extraction process by the previous layers, there will be one or more fully connected layers, also known as dense layers, which typically have the same number of output nodes as the number of classes and will perform high-level reasoning by building full connections to all activations from previous layers. The softmax function is often attached into the fully connected layer, which is a function that transforms a vector of $K$ real values into a vector of $K$ real values that sum to $1$. By adding fully connected layers, the network is able to learn a possibly non-linear function in the space of features generated by previous operations. ","date":"2020-10-01","objectID":"/posts/convolutional-neural-network/:3:0","tags":null,"title":"Convolutional Neural Network","uri":"/posts/convolutional-neural-network/"},{"categories":["Machine Learning"],"content":"4 What’s more The motivation for deep CNN architectures is based on the assumption that the deeper network is able to approximate the target function with more nonlinear mappings and diverse feature hierarchies and deeper networks can represent certain classes of function more efficiently than shallower ones. However, training a deeper networks is always a challenging task, and there have been a number of attempts trying to solve issues brought by deeper network such as gradient diminishing. Deeper CNN can theoretically perform well on complicated tasks. However, they may suffer issues brought by the increase of depth, including performance degradation, gradient vanishing, or explosion problems. ResNet, short for Residual Network, is one type of CNN that was firstly introduced in 2015, which revolutionised CNN by introducing the concept of residual learning in CNN training process to address the problems brought by deeper network. The most significant intuition behind ResNet is by introducing residual block which creates shortcut that can skip one or more layers. The reason why skip connection is introduced is that we want to overcome gradients vanishing by reusing activations from a previous layers so that the adjacent layers can learn weights. Apart from avoiding gradient vanishing, residual block will also speed the training process since skipping can reduce the network and lead to less parameter that require back propagation. Consequently, training much deeper networks becomes possible and the performance of deeper network is supposed to be at least as good as a shallower one. A Normal Block (left) and a Residual Block (right) As the figure indicates, there is an extra identity mapping from the input to the output so that the output $y_l$ becomes $f(x_l, w_l) + x_{l}$ instead of $f(x_l, w_l)$ by residual block, where $x_{l} = ReLU(y_{l-1})$. The intuition behind the residual block is that sometimes identity mapping is optimal and it would be difficult for a set of non-linear layers to fit an identity mapping so that the residual blocks make it easier to optimize the residual function $f(x_l, w_l) + x_{l}$ than $f(x_l, w_l)$. Moreover, the involvement of residual block also reduces the error rate brought by much deeper network structure. The reason for choosing ResNet over other popular CNN architectures is that ResNet can be more efficient as it requires less time in training with less parameters to learn and there are a variety of ResNet which is flexible for us to conduct further research. Since there is action recognition task on per-frame data, ResNet can be an effective approach. CNN has been developing in computer vision area in decades. One of the most popular approaches of CNN-based action recognition is two-stream CNN with 2D convolutional kernels, which combines RGB and incorporated optical flow frames as appearance and motion information. The efficiency of two-stream CNN is proved by increase of action recognition accuracy. Thereafter, a number of methods based on the two-stream CNN are proposed to improve action recognition performance. CNN with spatial-temporal 3D convolutional kernels, which can learn spatial-temporal feature representation from raw videos, was actually proposed many years ago but it cannot achieve a compelling results as two-stream CNN does as a result of relatively small-scale of existing video datasets for optimizing the huge number of parameters in 3D CNN, which are much more than those in a 2D CNN. With more large-scale video datasets released, 3D CNN has recently achieved impressive performance on a variety of tasks based on large-scale datasets. Despite training a 3D CNN requires more computation resources and can be difficult to optimize, the use of recent large-scale video datasets enables the training and significantly improves their performance. 3D ResNet is proposed based on ResNet. The main difference is that 3D ResNet has different number of dimensions of convolutional kernels a","date":"2020-10-01","objectID":"/posts/convolutional-neural-network/:4:0","tags":null,"title":"Convolutional Neural Network","uri":"/posts/convolutional-neural-network/"},{"categories":null,"content":" Geometry Texture Structure Geometry Palette Fabric Motion Aerial Blue Structure Geometry Texture Geometry Symmetry ","date":"0001-01-01","objectID":"/photography/abstract/:0:0","tags":null,"title":"Abstract","uri":"/photography/abstract/"},{"categories":null,"content":" 2022 2022 2022 2022 2022 2023 2024 2024 2024 2024 2024 2024 2024 2024 2025 ","date":"0001-01-01","objectID":"/photography/activity/:0:0","tags":null,"title":"Activity","uri":"/photography/activity/"},{"categories":null,"content":" 2021 2021 2022 2022 2022 2022 2022 2022 2023 2023 2023 2023 2023 2023 2024 2024 2024 2024 2024 2024 ","date":"0001-01-01","objectID":"/photography/natura/:0:0","tags":null,"title":"Natura","uri":"/photography/natura/"},{"categories":null,"content":" 2018 2024 2020 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2023 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 2025 ","date":"0001-01-01","objectID":"/photography/noir/:0:0","tags":null,"title":"Noir","uri":"/photography/noir/"},{"categories":null,"content":" 2022 2022 2023 2023 2023 2024 2023 2023 2023 2023 2023 2023 2024 2024 2024 2024 2024 2024 ","date":"0001-01-01","objectID":"/photography/portrait/:0:0","tags":null,"title":"Portrait","uri":"/photography/portrait/"},{"categories":null,"content":" 2021 2021 2023 2021 2021 2022 2022 2022 2022 2022 2022 2022 2022 2023 2023 2023 2023 2024 2024 2024 2024 2024 2024 2024 2024 2024 ","date":"0001-01-01","objectID":"/photography/street/:0:0","tags":null,"title":"Street","uri":"/photography/street/"},{"categories":null,"content":"Some work in progress.. With a deep-rooted curiosity about the universe and a passion for technology, I thrive on exploring cutting-edge technological breakthroughs. In addition to my professional career, I dedicate my free time to various avocations that nurture my passions beyond my daily responsibilities, including photography, guitar, tennis and films. ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"This is not a self-intro","uri":"/about/"}]